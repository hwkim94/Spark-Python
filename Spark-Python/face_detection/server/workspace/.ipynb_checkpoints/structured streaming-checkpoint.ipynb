{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "        <div>\n",
       "            <p><b>SparkContext</b></p>\n",
       "\n",
       "            <p><a href=\"http://ip-172-31-28-32.ap-northeast-2.compute.internal:4040\">Spark UI</a></p>\n",
       "\n",
       "            <dl>\n",
       "              <dt>Version</dt>\n",
       "                <dd><code>v2.3.2</code></dd>\n",
       "              <dt>Master</dt>\n",
       "                <dd><code>local[*]</code></dd>\n",
       "              <dt>AppName</dt>\n",
       "                <dd><code>PySparkShell</code></dd>\n",
       "            </dl>\n",
       "        </div>\n",
       "        "
      ],
      "text/plain": [
       "<SparkContext master=local[*] appName=PySparkShell>"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from pyspark import SparkContext, SparkConf, storagelevel\n",
    "from pyspark.streaming.context import StreamingContext\n",
    "from pyspark.streaming.kafka import KafkaUtils\n",
    "\n",
    "from pyspark.sql import SparkSession\n",
    "import pyspark.sql.functions as psf\n",
    "from pyspark.sql.types import StructType, FloatType, IntegerType\n",
    "\n",
    "sc"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 연결 확인\n",
    "- https://spark.apache.org/docs/latest/structured-streaming-kafka-integration.html\n",
    "- 시작 : pyspark --packages org.apache.spark:spark-sql-kafka-0-10_2.11:2.3.2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark = SparkSession.builder \\\n",
    "                    .appName(\"wordcount\") \\\n",
    "                    .master(\"local[*]\") \\\n",
    "                    .getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "lines = spark \\\n",
    "  .readStream \\\n",
    "  .format(\"kafka\") \\\n",
    "  .option(\"kafka.bootstrap.servers\", \"localhost:9092\") \\\n",
    "  .option(\"subscribe\", \"stream1\") \\\n",
    "  .load()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "words = lines.selectExpr(\"CAST(key AS STRING)\", \"CAST(value AS STRING)\")\n",
    "result = words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 모든 것을 다 보려면complete\n",
    "query = result \\\n",
    "        .writeStream \\\n",
    "        .outputMode(\"update\") \\\n",
    "        .format(\"console\") \\\n",
    "        .start()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "try :\n",
    "    query.awaitTermination()\n",
    "except :\n",
    "    query.stop()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# stream 처리 skelleton\n",
    "- https://spark.apache.org/docs/2.3.0/structured-streaming-kafka-integration.html\n",
    "- https://spark.apache.org/docs/2.3.0/structured-streaming-programming-guide.html\n",
    "- https://stackoverflow.com/questions/39235704/split-spark-dataframe-string-column-into-multiple-columns\n",
    "- https://spark.apache.org/docs/2.3.0/sql-programming-guide.html\n",
    "- https://databricks.com/blog/2017/02/23/working-complex-data-formats-structured-streaming-apache-spark-2-1.html\n",
    "- https://changhsinlee.com/pyspark-udf/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark = SparkSession.builder \\\n",
    "                    .appName(\"skelleton\") \\\n",
    "                    .master(\"local[*]\") \\\n",
    "                    .getOrCreate()\n",
    "            \n",
    "source = spark \\\n",
    "        .readStream \\\n",
    "        .format(\"kafka\") \\\n",
    "        .option(\"kafka.bootstrap.servers\", \"localhost:9092\") \\\n",
    "        .option(\"subscribe\", \"stream1\") \\\n",
    "        .load()\n",
    "        \n",
    "######################################################################################\n",
    "\n",
    "line = source.selectExpr(\"CAST(key AS STRING)\", \"CAST(value AS STRING)\") \\\n",
    "             .select(psf.col(\"key\"), psf.explode(psf.split(psf.col(\"value\"), \"\\n\")).alias(\"value\")) \\\n",
    "             .select(psf.col(\"key\"), psf.split(psf.col(\"value\"), \", \").alias(\"value\"))\n",
    "        \n",
    "######################################################################################\n",
    "\n",
    "df = line.select(psf.col(\"key\").alias(\"token\"),\n",
    "                 psf.col(\"value\").getItem(8).alias(\"time\"),\n",
    "                 psf.col(\"value\").getItem(0).alias(\"img_cnt\"),\n",
    "                 psf.col(\"value\").getItem(1).alias(\"face_cnt\"),\n",
    "                 psf.col(\"value\").getItem(2).alias(\"eye_cnt\"),\n",
    "                 psf.col(\"value\").getItem(3).alias(\"confidence\"),\n",
    "                 psf.col(\"value\").getItem(4).alias(\"x\"),\n",
    "                 psf.col(\"value\").getItem(5).alias(\"y\"),\n",
    "                 psf.col(\"value\").getItem(6).alias(\"w\"),\n",
    "                 psf.col(\"value\").getItem(7).alias(\"h\"))\\\n",
    "         .selectExpr(\"token\", \n",
    "                     \"time\",\n",
    "                     \"CAST(img_cnt AS Double)\", \n",
    "                     \"CAST(face_cnt AS Double)\",\n",
    "                     \"CAST(eye_cnt AS Double)\", \n",
    "                     \"CAST(confidence AS Double)\",\n",
    "                     \"CAST(x AS Double)\", \n",
    "                     \"CAST(y AS Double)\",\n",
    "                     \"CAST(w AS Double)\", \n",
    "                     \"CAST(h AS Double)\")\n",
    "    \n",
    "######################################################################################\n",
    "\n",
    "confidence = df.select(psf.col(\"token\"), psf.col(\"time\"), psf.col(\"confidence\"))\n",
    "confidence_agg = df.select(psf.col(\"token\"), psf.col(\"time\"), psf.col(\"confidence\")) \\\n",
    "                   .agg(psf.min(\"confidence\").alias(\"confidence_min\"), \n",
    "                        psf.mean(\"confidence\").alias(\"confidence_mean\"), \n",
    "                        psf.count(\"token\").alias(\"count\"))\n",
    "\n",
    "######################################################################################\n",
    "\n",
    "def dist(x, y, w, h) :\n",
    "    center_x = x + w/2\n",
    "    center_y = y + h/2\n",
    "    \n",
    "    return ((center_x-240)**2 + (center_y-180)**2)**0.5\n",
    "\n",
    "def area(w, h) :\n",
    "    return w*h\n",
    "\n",
    "dist_udf = psf.udf(lambda x,y,w,h: dist(x,y,w,h), FloatType())\n",
    "area_udf = psf.udf(lambda w,h : area(w,h), FloatType())\n",
    "\n",
    "distance = df.select(psf.col(\"token\"), psf.col(\"time\"), psf.col(\"x\"), psf.col(\"y\"), psf.col(\"w\"), psf.col(\"h\")) \\\n",
    "             .select(psf.col(\"token\"), \n",
    "                     psf.col(\"time\"),\n",
    "                     psf.col(\"x\"),\n",
    "                     psf.col(\"y\"),\n",
    "                     psf.col(\"w\").alias(\"wh\"),\n",
    "                     dist_udf(\"x\", \"y\", \"w\", \"h\").alias(\"dist\"), \n",
    "                     area_udf(\"w\", \"h\").alias(\"area\"))\n",
    "distance_agg = df.select(psf.col(\"token\"), psf.col(\"time\"), psf.col(\"x\"), psf.col(\"y\"), psf.col(\"w\"), psf.col(\"h\")) \\\n",
    "                 .agg(psf.mean(\"x\").alias(\"x_mean\"), \n",
    "                      psf.mean(\"y\").alias(\"y_mean\"), \n",
    "                      psf.mean(\"w\").alias(\"w_mean\"), \n",
    "                      psf.mean(\"h\").alias(\"h_mean\"))\n",
    "    \n",
    "######################################################################################\n",
    "\n",
    "def div(a, b) :\n",
    "    if a == 0 :\n",
    "        return 0\n",
    "    else :\n",
    "        return b/a\n",
    "    \n",
    "div_udf = psf.udf(lambda a,b : div(a,b), FloatType())\n",
    "\n",
    "cnt = df.select(psf.col(\"token\"), psf.col(\"time\"), psf.col(\"img_cnt\"), psf.col(\"face_cnt\"), psf.col(\"eye_cnt\")) \\\n",
    "        .select(psf.col(\"token\"), \n",
    "                psf.col(\"time\"),\n",
    "                psf.col(\"img_cnt\"), \n",
    "                div_udf(\"img_cnt\", \"face_cnt\").alias(\"face_ratio\"), \n",
    "                div_udf(\"img_cnt\", \"eye_cnt\").alias(\"eye_ratio\"))\n",
    "cnt_agg = df.select(psf.col(\"token\"), psf.col(\"img_cnt\"), psf.col(\"face_cnt\"), psf.col(\"eye_cnt\")) \\\n",
    "            .agg(psf.mean(\"img_cnt\").alias(\"img_cnt_mean\"), \n",
    "                 psf.mean(\"face_cnt\").alias(\"face_cnt_mean\"), \n",
    "                 psf.mean(\"eye_cnt\").alias(\"eye_cnt_mean\"))\n",
    "\n",
    "######################################################################################\n",
    "\n",
    "joined_df = confidence.join(distance, [\"token\",\"time\"])\n",
    "joined_df = joined_df.join(cnt, [\"token\",\"time\"])\n",
    "\n",
    "######################################################################################\n",
    "# join은 append 모드에서만 가능\n",
    "# join이 없을 경우에는 update를 사용하면 새로 들어오는 값의 straming 처리를 확인할 수 있음\n",
    "\n",
    "result = joined_df\n",
    "query = result \\\n",
    "        .writeStream \\\n",
    "        .outputMode(\"append\") \\\n",
    "        .format(\"console\") \\\n",
    "        .start()\n",
    "        \n",
    "######################################################################################  \n",
    "\n",
    "try :\n",
    "    query.awaitTermination()\n",
    "except :\n",
    "    query.stop()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# stream 처리1 \n",
    "- window를 사용하지 않고 각 수집시기마다 처리\n",
    "- 각 사용자의 데이터마다 처리하기 때문에 groupBy 사용하지 않음\n",
    "- stream-static join 사용\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark = SparkSession.builder \\\n",
    "                    .appName(\"stream_without_window\") \\\n",
    "                    .master(\"local[*]\") \\\n",
    "                    .getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+---------------+-------------------+--------------+-------------------+-----------------+-----------------+---------+------------------+------------------+-----------------+------+------------------+------+------------------+-------+------------------+\n",
      "|     token|face_ratio_mean|     face_ratio_std|eye_ratio_mean|      eye_ratio_std|  confidence_mean|   confidence_std|area_mean|          area_std|         dist_mean|         dist_std|x_mean|             x_std|y_mean|             y_std|wh_mean|            wh_std|\n",
      "+----------+---------------+-------------------+--------------+-------------------+-----------------+-----------------+---------+------------------+------------------+-----------------+------+------------------+------+------------------+-------+------------------+\n",
      "|0123456789|           0.76|0.07516648189186452|       0.35875|0.04250000000000002|38.34141864776611|4.604178031144208| 5216.835|196.50912691221015|20.713182306289674|0.409143507442128|196.86|1.0225596025188444|124.42|0.7852919311220703| 72.215|1.3557156132908503|\n",
      "+----------+---------------+-------------------+--------------+-------------------+-----------------+-----------------+---------+------------------+------------------+-----------------+------+------------------+------+------------------+-------+------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "static_df = spark.read.option(\"header\", \"true\") \\\n",
    "                      .csv(\"/hyunwoo/train_result/result.csv\") \\\n",
    "                      .selectExpr(\"token\",\n",
    "                                  \"CAST(face_ratio_mean AS Double)\",\n",
    "                                  \"CAST(face_ratio_std AS Double)\",\n",
    "                                  \"CAST(eye_ratio_mean AS Double)\",\n",
    "                                  \"CAST(eye_ratio_std AS Double)\",\n",
    "                                  \"CAST(confidence_mean AS Double)\",\n",
    "                                  \"CAST(confidence_std AS Double)\",\n",
    "                                  \"CAST(area_mean AS Double)\",\n",
    "                                  \"CAST(area_std AS Double)\",\n",
    "                                  \"CAST(dist_mean AS Double)\",\n",
    "                                  \"CAST(dist_std AS Double)\",\n",
    "                                  \"CAST(x_mean AS Double)\",\n",
    "                                  \"CAST(x_std AS Double)\",\n",
    "                                  \"CAST(y_mean AS Double)\",\n",
    "                                  \"CAST(y_std AS Double)\",\n",
    "                                  \"CAST(wh_mean AS Double)\",\n",
    "                                  \"CAST(wh_std AS Double)\")\n",
    "static_df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "source = spark \\\n",
    "        .readStream \\\n",
    "        .format(\"kafka\") \\\n",
    "        .option(\"kafka.bootstrap.servers\", \"localhost:9092\") \\\n",
    "        .option(\"subscribe\", \"stream1\") \\\n",
    "        .load()\n",
    "        \n",
    "######################################################################################\n",
    "\n",
    "line = source.selectExpr(\"CAST(key AS STRING)\", \"CAST(value AS STRING)\") \\\n",
    "             .select(psf.col(\"key\"), psf.explode(psf.split(psf.col(\"value\"), \"\\n\")).alias(\"value\")) \\\n",
    "             .select(psf.col(\"key\"), psf.split(psf.col(\"value\"), \", \").alias(\"value\"))\n",
    "        \n",
    "######################################################################################\n",
    "\n",
    "df = line.select(psf.col(\"key\").alias(\"token\"),\n",
    "                 psf.col(\"value\").getItem(8).alias(\"time\"),\n",
    "                 psf.col(\"value\").getItem(0).alias(\"img_cnt\"),\n",
    "                 psf.col(\"value\").getItem(1).alias(\"face_cnt\"),\n",
    "                 psf.col(\"value\").getItem(2).alias(\"eye_cnt\"),\n",
    "                 psf.col(\"value\").getItem(3).alias(\"confidence\"),\n",
    "                 psf.col(\"value\").getItem(4).alias(\"x\"),\n",
    "                 psf.col(\"value\").getItem(5).alias(\"y\"),\n",
    "                 psf.col(\"value\").getItem(6).alias(\"w\"),\n",
    "                 psf.col(\"value\").getItem(7).alias(\"h\"))\\\n",
    "         .selectExpr(\"token\", \n",
    "                     \"time\",\n",
    "                     \"CAST(img_cnt AS Double)\", \n",
    "                     \"CAST(face_cnt AS Double)\",\n",
    "                     \"CAST(eye_cnt AS Double)\", \n",
    "                     \"CAST(confidence AS Double)\",\n",
    "                     \"CAST(x AS Double)\", \n",
    "                     \"CAST(y AS Double)\",\n",
    "                     \"CAST(w AS Double)\", \n",
    "                     \"CAST(h AS Double)\")\n",
    "    \n",
    "######################################################################################\n",
    "\n",
    "def dist(x, y, w, h) :\n",
    "    center_x = x + w/2\n",
    "    center_y = y + h/2\n",
    "    \n",
    "    return ((center_x-240)**2 + (center_y-180)**2)**0.5\n",
    "\n",
    "def area(w, h) :\n",
    "    return w*h\n",
    "\n",
    "def div(a, b) :\n",
    "    if a == 0 :\n",
    "        return 0\n",
    "    else :\n",
    "        return b/a\n",
    "\n",
    "def normalize_and_map(value, mean, std) :\n",
    "    result = abs((value-mean)/std)-1.3\n",
    "    \n",
    "    if result >= 0 :\n",
    "        return 1\n",
    "    else :\n",
    "        return 0\n",
    "\n",
    "def overlap(x,xm,y,ym,wh,whm) :\n",
    "    if x == 0 :\n",
    "        return 0\n",
    "    else : \n",
    "        r1 = (x, y-wh, x+wh, y)\n",
    "        r2 = (xm , ym-whm, xm+whm, ym)\n",
    "        \n",
    "        dx = min(r1[2], r2[2]) - max(r1[0], r2[0])\n",
    "        dy = min(r1[3], r2[3]) - max(r1[1], r2[1])\n",
    "        area = dx*dy\n",
    "        \n",
    "        if (dx>=0) and (dy>=0) and (area/(whm*whm) >= 0.6) :\n",
    "            return 0\n",
    "        else :\n",
    "            return 1\n",
    "        \n",
    "def get_result(a,b,c,d,e,f) :\n",
    "    result = a+b+c+d+e+f\n",
    "    if result >=4 :\n",
    "        return 0\n",
    "    else :\n",
    "        return 1\n",
    "    \n",
    "dist_udf = psf.udf(lambda x,y,w,h: dist(x,y,w,h), FloatType())\n",
    "area_udf = psf.udf(lambda w,h : area(w,h), FloatType())\n",
    "div_udf = psf.udf(lambda a,b : div(a,b), FloatType())\n",
    "normal_map_udf = psf.udf(lambda a,b,c : normalize_and_map(a,b,c), IntegerType())\n",
    "overlap_udf = psf.udf(lambda x,xm,y,ym,wh,whm: overlap(x,xm,y,ym,wh,whm), IntegerType())\n",
    "result_udf = psf.udf(lambda a,b,c,d,e,f, : get_result(a,b,c,d,e,f), IntegerType())\n",
    "\n",
    "######################################################################################\n",
    "\n",
    "stream_df = df.select(psf.col(\"token\"), \n",
    "                      psf.col(\"time\"), \n",
    "                      psf.col(\"confidence\"),\n",
    "                      psf.col(\"x\"),\n",
    "                      psf.col(\"y\"),\n",
    "                      psf.col(\"w\").alias(\"wh\"),\n",
    "                      dist_udf(\"x\", \"y\", \"w\", \"h\").alias(\"dist\"), \n",
    "                      area_udf(\"w\", \"h\").alias(\"area\"),\n",
    "                      div_udf(\"img_cnt\", \"face_cnt\").alias(\"face_ratio\"), \n",
    "                      div_udf(\"img_cnt\", \"eye_cnt\").alias(\"eye_ratio\"))\n",
    "        \n",
    "joined_df = stream_df.join(static_df, [\"token\"])\n",
    "\n",
    "normal_df = joined_df.select(psf.col(\"token\"), \n",
    "                             psf.col(\"time\"), \n",
    "                             normal_map_udf(\"confidence\", \"confidence_mean\", \"confidence_std\").alias(\"confidence\"),\n",
    "                             overlap_udf(\"x\", \"x_mean\", \"y\", \"y_mean\", \"wh\", \"wh_mean\").alias(\"overlap\"),\n",
    "                             normal_map_udf(\"dist\", \"dist_mean\", \"dist_std\").alias(\"dist\"),\n",
    "                             normal_map_udf(\"area\", \"area_mean\", \"area_std\").alias(\"area\"),\n",
    "                             normal_map_udf(\"face_ratio\", \"face_ratio_mean\", \"face_ratio_std\").alias(\"face_ratio\"),\n",
    "                             normal_map_udf(\"eye_ratio\", \"eye_ratio_mean\", \"eye_ratio_std\").alias(\"eye_ratio\"))\n",
    "\n",
    "result_df = normal_df.select(psf.col(\"token\"), \n",
    "                             psf.col(\"time\"),\n",
    "                             result_udf(\"confidence\", \"overlap\", \"dist\", \"area\", \"face_ratio\", \"eye_ratio\").alias(\"result\"))\n",
    "\n",
    "######################################################################################\n",
    "# join은 append 모드에서만 가능\n",
    "# join이 없을 경우에는 update를 사용하면 새로 들어오는 값의 straming 처리를 확인할 수 있음\n",
    "\n",
    "result = result_df.selectExpr(\"CAST(token AS STRING) AS key\", \"to_json(struct(*)) AS value\")\n",
    "query = result \\\n",
    "        .writeStream \\\n",
    "        .format(\"kafka\") \\\n",
    "        .option(\"kafka.bootstrap.servers\", \"localhost:9092\") \\\n",
    "        .option(\"checkpointLocation\", \"/hyunwoo/stream1_checkpoint\") \\\n",
    "        .option(\"topic\", \"stream2\") \\\n",
    "        .start()\n",
    "        \n",
    "######################################################################################  \n",
    "\n",
    "try :\n",
    "    query.awaitTermination()\n",
    "except :\n",
    "    query.stop()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
